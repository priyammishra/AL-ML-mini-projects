{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "0.7833333333333333\n",
      "0.7704918032786885\n",
      "KNN\n",
      "0.6274509803921569\n",
      "0.5245901639344263\n",
      "Bayes\n",
      "0.8035714285714286\n",
      "0.7377049180327869\n"
     ]
    }
   ],
   "source": [
    "# We're trying to solve binary classifiction problem. Till now, we have discussed three models to solve binary classification problem -\n",
    "# Logistic regression, KNN, Bayes\n",
    "# We're going to train the data on all three and then see which is the best suited for this problem\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "loan_df = pd.read_csv(\"loan_approval_data.csv\")\n",
    "# loan_df.head()\n",
    "\n",
    "# Data cleaning - handle missing values, df will automatically fill missing value with NaN s we need to handle NaN\n",
    "# Fill numerical value columns with it's mean value\n",
    "\n",
    "numeric_cols = loan_df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    if loan_df[col].isnull().any(): # Check if the column actually has NaNs to fill\n",
    "        col_mean = loan_df[col].mean()\n",
    "        loan_df[col] = loan_df[col].fillna(col_mean)\n",
    "\n",
    "# Fill non-numerical column values with it's mode(most-frequent)\n",
    "non_numeric_cols = loan_df.select_dtypes(include=['object', 'category']).columns\n",
    "for col in non_numeric_cols:\n",
    "    # Calculate the mode and take the first value (as mode can return multiple values)\n",
    "    col_mode = loan_df[col].mode()[0]\n",
    "    # Fill NaN values in the column with the calculated mode\n",
    "    loan_df[col] = loan_df[col].fillna(col_mode)\n",
    "\n",
    "# We can use skleanr SimpleImputer also to fill NaN values\n",
    "# To check if all the columns have null values it will return 0 if there are no null values\n",
    "loan_df.isnull().sum()\n",
    "\n",
    "# Next step is to check for outliers by plotting the data\n",
    "# In this data set there are no such outliers\n",
    "# In case we have outliers, then we have to remove it but not all outliers have to be removed\n",
    "# We remove two types of outliers - \n",
    "# 1. Illogical/Impossible/wrong values - eg if age is in negative or age is 10 years r 150 years or dependent value is -1\n",
    "# 2. No meaningful information\n",
    "\n",
    "# Remove applicant id because this doesn't have to do woth loan approval\n",
    "loan_df = loan_df.drop(\"Applicant_ID\", axis=1)\n",
    "loan_df\n",
    "\n",
    "# Feature engineering - encoding - update non-numerical value to numerical value\n",
    "# We use LabelEncoder - ordinal data and ouptut data and OneHotEncoder for nominal data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "loan_df[\"Education_Level\"] = le.fit_transform(loan_df[\"Education_Level\"])\n",
    "loan_df[\"Loan_Approved\"] = le.fit_transform(loan_df[\"Loan_Approved\"])\n",
    "\n",
    "cols = [\"Employment_Status\", \"Marital_Status\", \"Loan_Purpose\", \"Property_Area\", \"Gender\", \"Employer_Category\"]\n",
    "ohe = OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"ignore\")\n",
    "encoded = ohe.fit_transform(loan_df[cols]) # This will return encoded value in a 2d list so we need to create a dataframe\n",
    "\n",
    "encoded_df = pd.DataFrame(encoded, columns=ohe.get_feature_names_out(cols), index=loan_df.index)\n",
    "loan_df = pd.concat([loan_df.drop(columns=cols), encoded_df], axis=1)\n",
    "\n",
    "# Split data\n",
    "X = loan_df.drop(\"Loan_Approved\", axis=1)\n",
    "y = loan_df[\"Loan_Approved\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train and evaluate models\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = log_model.predict(X_test_scaled)\n",
    "# Evaluation\n",
    "# False positive we need to reduce so here we consider precision and then recall(False negative)\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(\"Logistic regression\")\n",
    "ps_score = precision_score(y_test, y_pred)\n",
    "print(ps_score)\n",
    "rc_score = recall_score(y_test, y_pred)\n",
    "print(rc_score)\n",
    "\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = knn_model.predict(X_test_scaled)\n",
    "# Evaluation\n",
    "# False positive we need to reduce so here we consider precision and then recall(False negative)\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(\"KNN\")\n",
    "ps_score = precision_score(y_test, y_pred)\n",
    "print(ps_score)\n",
    "rc_score = recall_score(y_test, y_pred)\n",
    "print(rc_score)\n",
    "\n",
    "# Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "bayes_model = GaussianNB()\n",
    "bayes_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = bayes_model.predict(X_test_scaled)\n",
    "# Evaluation\n",
    "# False positive we need to reduce so here we consider precision and then recall(False negative)\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(\"Bayes\")\n",
    "ps_score = precision_score(y_test, y_pred)\n",
    "print(ps_score);\n",
    "rc_score = recall_score(y_test, y_pred)\n",
    "print(rc_score);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
